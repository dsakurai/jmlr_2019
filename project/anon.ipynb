{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "\n",
    "\n",
    "import core.config as config\n",
    "from chofer_tda_datasets import Anon10kEigenvaluePredict\n",
    "from core.utils import *\n",
    "\n",
    "from torchph.nn.slayer import SLayerExponential, \\\n",
    "SLayerRational, \\\n",
    "LinearRationalStretchedBirthLifeTimeCoordinateTransform, \\\n",
    "prepare_batch, SLayerRationalHat\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from collections import Counter, defaultdict\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(1)\n",
    "\n",
    "\n",
    "class AnonCollate:   \n",
    "    def __init__(self, cuda=True):\n",
    "        self.cuda = cuda\n",
    "        \n",
    "    def __call__(self, sample_target_iter):\n",
    "        x, y = [], []\n",
    "        for x_i, y_i in sample_target_iter:\n",
    "            x.append(x_i)\n",
    "            y.append(y_i)\n",
    "\n",
    "        x = prepare_batch(x, 2)            \n",
    "        y = torch.Tensor(y)\n",
    "\n",
    "        if self.cuda:\n",
    "            # Shifting the necessary parts of the prepared batch to the cuda\n",
    "            x = (x[0].cuda(), x[1].cuda(), x[2], x[3])\n",
    "            y = y.cuda()\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "class train_env:\n",
    "    n_epochs = 100\n",
    "    lr_initial = 0.5\n",
    "    lr_epoch_step = 10\n",
    "    batch_size = 64\n",
    "    train_size = 0.9\n",
    "    nu = 0.01\n",
    "    n_target_bins = 100\n",
    "\n",
    "    \n",
    "dataset = Anon10kEigenvaluePredict(data_root_folder_path=config.paths.data_root_dir)\n",
    "dataset.keys_essential = ('dim_0_ess', 'dim_1_ess')\n",
    "dataset.keys_not_essential = ('dim_0',)\n",
    "\n",
    "\n",
    "coordinate_transform  = LinearRationalStretchedBirthLifeTimeCoordinateTransform(nu=train_env.nu)    \n",
    "\n",
    "        \n",
    "dataset.data_transforms = [\n",
    "    lambda x: x['dim_0'][()],\n",
    "    lambda x: torch.from_numpy(x).float(), \n",
    "    coordinate_transform\n",
    "]\n",
    "\n",
    "\n",
    "def histogramize(x):\n",
    "    return np.histogram(x, density=True, bins=train_env.n_target_bins,range=(0,2))[0].tolist()\n",
    "    \n",
    "\n",
    "dataset.target_transforms = [histogramize]                      \n",
    "reddit_collate = AnonCollate(cuda=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearCell(n_in, n_out):\n",
    "    m = nn.Sequential(nn.Linear(n_in, n_out), \n",
    "                      nn.BatchNorm1d(n_out), \n",
    "                      nn.ReLU())\n",
    "    m.out_features = m[0].out_features\n",
    "    return m\n",
    "\n",
    "\n",
    "def Slayer(n_elements, point_dim):\n",
    "    return SLayerRationalHat(n_elements, point_dimension=2, radius_init=250)   \n",
    "\n",
    "\n",
    "class AnonModel(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        \n",
    "        dim_0_n_elements = 100        \n",
    "        self.dim_0 = Slayer(dim_0_n_elements, 2)          \n",
    "\n",
    "        self.regressor = nn.Sequential(  \n",
    "                                         nn.Tanh(),\n",
    "                                         LinearCell(dim_0_n_elements, train_env.n_target_bins),\n",
    "                                         LinearCell(train_env.n_target_bins,train_env.n_target_bins),                                   \n",
    "                                         nn.Linear(train_env.n_target_bins, train_env.n_target_bins))\n",
    "                         \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.dim_0(x)\n",
    "        x = self.regressor(x)        \n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def centers_init(self, sample_target_iter):   \n",
    "        x = [x for x, _ in sample_target_iter]\n",
    "        dim_0 = torch.cat(x, dim=0)\n",
    "        dim_0 = list({tuple(row) for row in dim_0})\n",
    "        dim_0 = np.array(dim_0)\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=self.dim_0.centers.size(0), \n",
    "                                        init='k-means++', \n",
    "                                        random_state=123, \n",
    "                                        n_jobs=10, \n",
    "                                        n_init=2, )                           \n",
    "        kmeans.fit(dim_0)\n",
    "        centers = kmeans.cluster_centers_\n",
    "        centers = torch.from_numpy(centers).float()\n",
    "        self.dim_0.centers.data = centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def experiment():      \n",
    "    stats_of_runs = []\n",
    "    splitter = ShuffleSplit(n_splits=10, \n",
    "                            train_size=train_env.train_size, \n",
    "                            test_size=1-train_env.train_size, \n",
    "                            random_state=123)\n",
    "    \n",
    "    train_test_splits = list(splitter.split(X=dataset.targets, y=dataset.targets))\n",
    "    train_test_splits = [(train_i.tolist(), test_i.tolist()) for train_i, test_i in train_test_splits]\n",
    "    \n",
    "    for run_i, (train_i, test_i) in enumerate(train_test_splits):\n",
    "        print('')\n",
    "        print('Run', run_i)\n",
    "        \n",
    "        model = AnonModel()\n",
    "#         model.centers_init([dataset[i] for i in train_i])\n",
    "        model.cuda()\n",
    "\n",
    "        stats = defaultdict(list)\n",
    "        stats_of_runs.append(stats)\n",
    "        \n",
    "        opt=torch.optim.SGD(model.parameters(), lr=train_env.lr_initial, momentum=0.9)\n",
    "\n",
    "        for i_epoch in range(1, train_env.n_epochs+1):      \n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            dl_train = DataLoader(dataset,\n",
    "                              batch_size=train_env.batch_size, \n",
    "                              collate_fn=reddit_collate,\n",
    "                              sampler=SubsetRandomSampler(train_i),\n",
    "                              num_workers=5)\n",
    "\n",
    "            dl_test = DataLoader(dataset,\n",
    "                                 batch_size=train_env.batch_size, \n",
    "                                 collate_fn=reddit_collate, \n",
    "                                 sampler=SubsetRandomSampler(test_i),\n",
    "                                 num_workers=5)\n",
    "\n",
    "            epoch_loss = 0\n",
    "\n",
    "            if i_epoch % train_env.lr_epoch_step == 0:\n",
    "                adapt_lr(opt, lambda lr: lr*0.5)\n",
    "\n",
    "            for i_batch, (x, y) in enumerate(dl_train, 1):\n",
    "                \n",
    "                x = (x[0].cuda(), x[1].cuda(), x[2], x[3])\n",
    "                y = y.cuda()\n",
    "                \n",
    "                y = torch.autograd.Variable(y)\n",
    "\n",
    "                def closure():\n",
    "                    opt.zero_grad()\n",
    "                    y_hat = model(x)            \n",
    "                    loss = histogram_intersection_loss(y_hat, y)   \n",
    "                    \n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "\n",
    "                loss = opt.step(closure)\n",
    "\n",
    "                epoch_loss += float(loss)\n",
    "                stats['loss_by_batch'].append(float(loss))\n",
    "                stats['centers'].append(model.dim_0.centers.data.cpu().numpy())\n",
    "\n",
    "                print(\"Epoch {}/{}, Batch {}/{}\".format(i_epoch, train_env.n_epochs, i_batch, len(dl_train)), end=\"       \\r\")\n",
    "\n",
    "            stats['train_loss_by_epoch'].append(epoch_loss/len(dl_train))\n",
    "\n",
    "            \n",
    "            # last epoch dump test results           \n",
    "            if i_epoch == train_env.n_epochs:\n",
    "                y_true = []\n",
    "                y_pred = []\n",
    "                for i_batch, (x, y_true_i) in enumerate(dl_test):    \n",
    "                    x = (x[0].cuda(), x[1].cuda(), x[2], x[3])\n",
    "                    y_pred_i = model(x)\n",
    "                    \n",
    "                    y_true.append(y_true_i.cpu())\n",
    "                    y_pred.append(y_pred_i.data.cpu())\n",
    "                    \n",
    "                y_true = torch.cat(y_true, dim=0)\n",
    "                y_pred = torch.cat(y_pred, dim=0)\n",
    "                \n",
    "                stats['y_true'] = y_true\n",
    "                stats['y_pred'] = y_pred \n",
    "                \n",
    "                stats['test_histogram_intersection'] = -histogram_intersection_loss(y_pred, y_true, reduce=False).numpy()\n",
    "                print('')\n",
    "                \n",
    "        stats['model'] = model.cpu()\n",
    "    return stats_of_runs\n",
    "stats_of_runs = experiment()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_histogram_intersections = [np.array(r['test_histogram_intersection']).mean() for r in stats_of_runs]\n",
    "print(np.mean(test_histogram_intersections))\n",
    "print(np.std(test_histogram_intersections))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
