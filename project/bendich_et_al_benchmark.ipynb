{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.config as config\n",
    "\n",
    "\n",
    "from chofer_tda_datasets import Reininghaus2014ShrecReal, SciNe01EEGBottomTopFiltration\n",
    "from chofer_tda_datasets.transforms import Hdf5GroupToDict, Hdf5GroupToDictSelector\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def bendich_vectorization(dgm, num_dims=100):\n",
    "    persistences = [d-b for b, d in dgm]\n",
    "    v = sorted(persistences, reverse=True)\n",
    "    if len(v) < num_dims:\n",
    "        v += [0]*(num_dims - len(v))\n",
    "        \n",
    "    return v[:num_dims]\n",
    "\n",
    "\n",
    "def svm_linear_standard_scaled_c_optimized(pca_num_dims=None):\n",
    "    grid = {'C': [0.1, 1, 10, 100]}\n",
    "    clf = GridSearchCV(cv=3, \n",
    "                       estimator=LinearSVC(),\n",
    "                       param_grid=grid, \n",
    "                       n_jobs=10\n",
    "                    )\n",
    "    \n",
    "    pipeline_members = []\n",
    "    pipeline_members.append(('scaler', StandardScaler()))\n",
    "    \n",
    "    if pca_num_dims is not None:\n",
    "        pipeline_members.append(('pca', PCA(pca_num_dims)))\n",
    "        \n",
    "    pipeline_members.append(('classifier', clf))\n",
    "    \n",
    "    return Pipeline(pipeline_members)\n",
    "\n",
    "\n",
    "def bendich_vectorization_generic_experiment(dataset, \n",
    "                                             vectorization_callback, \n",
    "                                             vectorization_dimensions, \n",
    "                                             pca_num_dims=None):\n",
    "    \n",
    "    \n",
    "    train_size = 0.9\n",
    "    \n",
    "    splitter = StratifiedShuffleSplit(n_splits=10, \n",
    "                                      train_size=train_size, \n",
    "                                      test_size=1-train_size, \n",
    "                                      random_state=123)\n",
    "    train_test_splits = list(splitter.split(X=dataset.targets, y=dataset.targets))\n",
    "    train_test_splits = [(train_i.tolist(), test_i.tolist()) for train_i, test_i in train_test_splits]\n",
    "\n",
    "\n",
    "    return_value = {}      \n",
    "    \n",
    "    X = []\n",
    "    y = []    \n",
    "    \n",
    "    for i, (x_i, y_i) in enumerate(dataset):\n",
    "        clear_output(wait=True)\n",
    "        print('loading data ... ', i, end='\\r')\n",
    "        sys.stdout.flush()\n",
    "        v = vectorization_callback(x_i, num_dims=max(vectorization_dimensions))\n",
    "        X.append(v)\n",
    "        y.append(int(y_i))\n",
    "        \n",
    "#     X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print('')\n",
    "    \n",
    "    for dim in vectorization_dimensions:\n",
    "        print('dimension =', dim, \":\")\n",
    "        \n",
    "        return_value_dim = defaultdict(list)\n",
    "        return_value[dim] = return_value_dim\n",
    "        \n",
    "        X_dim = []\n",
    "        for x in X:\n",
    "            X_dim.append(sum([v[:dim] for v in x], []))\n",
    "            \n",
    "        X_dim = np.array(X_dim)\n",
    "\n",
    "        for run_i, (train_i, test_i) in enumerate(train_test_splits):\n",
    "            print('run', run_i, end='\\r')\n",
    "            X_train = X_dim[train_i]\n",
    "            y_train = y[train_i]\n",
    "            \n",
    "            X_test = X_dim[test_i]\n",
    "            y_test = y[test_i]\n",
    "            \n",
    "            classifier = svm_linear_standard_scaled_c_optimized(pca_num_dims=pca_num_dims)                 \n",
    "            classifier.fit(X_train, y_train)            \n",
    "\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            return_value_dim['accuracies'].append(accuracy_score(y_test, y_pred))\n",
    "            \n",
    "        return_value_dim['classifier'].append(classifier)\n",
    "        print('')\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_shrec_real = Reininghaus2014ShrecReal(data_root_folder_path=config.paths.data_root_dir)\n",
    "ds_shrec_real.data_transforms = [Hdf5GroupToDict()]\n",
    "\n",
    "def shrec_real_bendich_vectorization(input_dict, num_dims):\n",
    "    ret_val = []\n",
    "    for scale in range(1, 11):\n",
    "        for dim in ['0', '1']:\n",
    "            x = input_dict[str(scale)][dim]\n",
    "            ret_val.append(bendich_vectorization(x, num_dims=num_dims))\n",
    "            \n",
    "    return ret_val\n",
    "\n",
    "shrec_result = bendich_vectorization_generic_experiment(ds_shrec_real, \n",
    "                                                        shrec_real_bendich_vectorization, \n",
    "                                                        vectorization_dimensions=[5, 10, 20, 40, 80, 160])\n",
    "\n",
    "\n",
    "with open('./bendich_exp_shrec_real.pickle', 'bw') as f:\n",
    "    pickle.dump(shrec_result, f)\n",
    "    \n",
    "for k, v in shrec_result.items():\n",
    "    print('dimension', k, 'accuracy:', np.mean(v['accuracies']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_scine_eeg = SciNe01EEGBottomTopFiltration(data_root_folder_path=config.paths.data_root_dir)\n",
    "sensor_indices = [str(i) for i in ds_scine_eeg.sensor_configurations['low_resolution_whole_head']]\n",
    "selection = {'top': sensor_indices, 'bottom': sensor_indices}\n",
    "selector = Hdf5GroupToDictSelector(selection)\n",
    "\n",
    "ds_scine_eeg.data_transforms = [selector]\n",
    "\n",
    "def scine_bendich_vectorization(input_dict, num_dims):\n",
    "    ret_val = []\n",
    "    for filt in ['top', 'bottom']:\n",
    "        for sensor_i in sensor_indices:\n",
    "            x = input_dict[filt][sensor_i]\n",
    "            ret_val.append(bendich_vectorization(x, num_dims=num_dims))\n",
    "            \n",
    "    return ret_val\n",
    "\n",
    "eeg_result = bendich_vectorization_generic_experiment(ds_scine_eeg, \n",
    "                                                   scine_bendich_vectorization, \n",
    "                                                   vectorization_dimensions=[5, 10, 20, 40, 80, 160],\n",
    "                                                   pca_num_dims=None)\n",
    "\n",
    "with open('./bendich_exp_scitrecs_eeg.pickle', 'bw') as f:\n",
    "    pickle.dump(eeg_result, f)\n",
    "    \n",
    "for k, v in eeg_result.items():\n",
    "    print('dimension', k, 'accuracy:', np.mean(v['accuracies']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bendich_exp_scitrecs_eeg.pickle', 'br') as f:\n",
    "    result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in result.items():\n",
    "    print(k, np.mean(v['accuracies']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
